<br>

<p align="center">
  <a href="https://haven.run"><img src="https://raw.githubusercontent.com/havenhq/haven/dev/logo.png" width="300"/></a>
</p>

<p align="center">
    <b>Fine-Tune and Deploy LLMs On Your Own Infrastructure</b>
</p>

<div align="center">

[💻 Quickstart](https://docs.haven.run/)
<span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>
[📄 Docs](https://docs.haven.run/)
<span>&nbsp;&nbsp;•&nbsp;&nbsp;</span>
[💬 Discord](https://discord.gg/JDjbfp6q2G)
<br>

<p align="center">
    Haven lets you build LLM-powered applications <b>hosted entirely on your own infrastructure</b>.<br>
    Just select a model to run - Haven will set up a production-ready 
  API server in your private cloud.
</p>

</div>

<br>

<p align="center">
  <img src="https://raw.githubusercontent.com/havenhq/haven/dev/diagram.svg">
</p>

## Getting Started 🔥

Setting up an LLM server requires just three steps:

1. Get an API key for a Google Cloud service account
2. Deploy Haven's manager container on a Google Cloud instance
3. Spin up a model worker using the Python SDK

A description of these steps can be found in our [documentation](https://docs.haven.run/).

<br>

## Roadmap 🚀

We're constantly building new features and would love your feedback! Here's what we are currently looking to integrate into our platform:

- [x] Inference Workers
- [x] Google Cloud Support
- [ ] Fine-Tuning Workers
- [ ] AWS Support

<br>

## Learn More 🔍

To learn more about our platform, you should refer to our [documentation](https://docs.haven.run/).
