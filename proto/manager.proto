syntax = "proto3";

package haven;

message Empty {}

service Haven {
	// Setup (first time starting the manager)
	rpc Setup(SetupRequest) returns (Empty) {}

	// Generate text from a prompt.
	rpc Generate(GenerateRequest) returns (stream GenerateResponse) {}

	// Get the list of models and their descriptions.
	rpc ListModels(Empty) returns (ListModelsResponse) {}

	// Model management.
	rpc AddModel(AddModelRequest) returns (Empty) {}
	rpc DeleteModel(ModelName) returns (Empty) {}

	// Inference worker management.
	rpc CreateInferenceWorker(CreateInferenceWorkerRequest) returns (InferenceWorker) {}
	rpc PauseInferenceWorker(InferenceWorker) returns (InferenceWorker) {}
	rpc ResumeInferenceWorker(InferenceWorker) returns (InferenceWorker) {}
	rpc DeleteInferenceWorker(InferenceWorker) returns (InferenceWorker) {}
}

message SetupRequest {
	optional string key_file = 1;
}

message GenerateRequest {
	string worker_name = 1;
	string prompt = 2;

	optional int32 max_tokens = 3;
	optional float temperature = 4;
	optional int32 top_p = 5;
	optional int32 top_k = 6;
	optional bool sample = 7;
}

message GenerateResponse {
	string text = 1;
}

enum Status {
	RUNNING = 0;
	STOPPED = 1; // Worker doesn't exist.
	STARTING = 2;
	STOPPING = 3; // When going to STOPPED or PAUSED.
	PAUSED = 4;
	ERROR = 5;
}

message Model {
	string name = 1;
}

message ListModelsResponse {
	repeated Model models = 1;
}

message AddModelRequest {
	string model_name = 1;
	string tokenizer_name = 2;
	string base_model_name = 3;
	string instruction_prefix = 4;
	string output_prefix = 5;
	repeated string stop_tokens = 6;
}

message ModelName {
	string model_name = 1;
}

enum GpuType {
	A100 = 0;
	A100_80GB = 1;
	T4 = 2;
}

message CreateInferenceWorkerRequest {
	string model_name = 1;
	string quantization = 2;
	optional string worker_name = 3;
	optional GpuType gpu_type = 4;
	optional int32 gpu_count = 6;
}

message InferenceWorker {
	string worker_id = 1;
}
