syntax = "proto3";

package haven;

message Empty {}

service Haven {
	// Setup (first time starting the manager)
	rpc Setup(SetupRequest) returns (SetupResponse) {}

	// Generate text.
	rpc ChatCompletion(ChatCompletionRequest) returns (stream CompletionResponse) {}
	rpc Completion(CompletionRequest) returns (stream CompletionResponse) {}

	// Get the list of models and their descriptions.
	rpc ListModels(Empty) returns (ListModelsResponse) {}
	rpc AddModel(Model) returns (Empty) {}
	rpc RemoveModel(ModelName) returns (Empty) {}

	// Get the list of workers and their statuses.
	rpc ListWorkers(Empty) returns (ListWorkersResponse) {}

	// Inference worker management.
	rpc CreateInferenceWorker(CreateInferenceWorkerRequest) returns (InferenceWorker) {}
	rpc PauseInferenceWorker(InferenceWorker) returns (InferenceWorker) {}
	rpc ResumeInferenceWorker(InferenceWorker) returns (InferenceWorker) {}
	rpc DeleteInferenceWorker(InferenceWorker) returns (InferenceWorker) {}
}

// Setup

message SetupRequest {
	optional string key_file = 1;
}

message SetupResponse {
	// Used for warnings when running on an outdated version.
	optional string message = 1;
}

// Completion

enum Role {
	ASSISTANT = 0;
	USER = 1;
}

message Message {
	Role role = 1;
	string content = 2;
}

message ChatCompletionRequest {
	string worker_name = 1;
	repeated Message messages = 2;

	optional int32 max_tokens = 3;
	optional float top_p = 4;
	optional int32 top_k = 5;
	optional float temperature = 6;
}

message CompletionRequest {
	string worker_name = 1;
	string prompt = 2;
	repeated string stop_tokens = 7;

	optional int32 max_tokens = 3;
	optional float top_p = 4;
	optional int32 top_k = 5;
	optional float temperature = 6;
}

message CompletionResponse {
	string text = 1;
}

// Models

message Model {
	string architecture = 2;
	string name = 1;
	string tokenizer = 3;

	optional string system_prompt = 4;
	optional string instruction_prefix = 5;
	optional string instruction_postfix = 6;
	optional string output_prefix = 7;
	optional string output_postfix = 8;
}

message ModelName {
	string name = 1;
}

message ListModelsResponse {
	repeated Model models = 1;
}

// Workers

enum Status {
	ONLINE = 0;
	LOADING = 1;
	PAUSED = 2;
	ERROR = 3;
}

message Worker {
	string worker_name = 1;
	Status status = 2;
}

message ListWorkersResponse {
	repeated Worker workers = 1;
}

enum GpuType {
	A100 = 0;
	A100_80GB = 1;
	T4 = 2;
}

message CreateInferenceWorkerRequest {
	string model_name = 1;
	string quantization = 2;
	optional string worker_name = 3;
	optional GpuType gpu_type = 4;
	optional int32 gpu_count = 6;
	optional string zone = 7;
}

message InferenceWorker {
	string worker_name = 1;
}
