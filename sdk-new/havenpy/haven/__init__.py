# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: proto/manager.proto
# plugin: python-betterproto
from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    AsyncIterator,
    Dict,
    List,
    Optional,
)

import betterproto
import grpclib
from betterproto.grpc.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpc.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


class Role(betterproto.Enum):
    ASSISTANT = 0
    USER = 1


class Status(betterproto.Enum):
    ONLINE = 0
    LOADING = 1
    PAUSED = 2
    ERROR = 3


class GpuType(betterproto.Enum):
    A100 = 0
    A100_80GB = 1
    T4 = 2


@dataclass(eq=False, repr=False)
class Empty(betterproto.Message):
    pass


@dataclass(eq=False, repr=False)
class SetupRequest(betterproto.Message):
    key_file: Optional[str] = betterproto.string_field(
        1, optional=True, group="_key_file"
    )


@dataclass(eq=False, repr=False)
class SetupResponse(betterproto.Message):
    message: Optional[str] = betterproto.string_field(
        1, optional=True, group="_message"
    )
    """Used for warnings when running on an outdated version."""


@dataclass(eq=False, repr=False)
class Message(betterproto.Message):
    role: "Role" = betterproto.enum_field(1)
    content: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class ChatCompletionRequest(betterproto.Message):
    worker_name: str = betterproto.string_field(1)
    messages: List["Message"] = betterproto.message_field(2)
    max_tokens: Optional[int] = betterproto.int32_field(
        3, optional=True, group="_max_tokens"
    )
    top_p: Optional[float] = betterproto.float_field(4, optional=True, group="_top_p")
    top_k: Optional[int] = betterproto.int32_field(5, optional=True, group="_top_k")
    temperature: Optional[float] = betterproto.float_field(
        6, optional=True, group="_temperature"
    )


@dataclass(eq=False, repr=False)
class CompletionRequest(betterproto.Message):
    worker_name: str = betterproto.string_field(1)
    prompt: str = betterproto.string_field(2)
    stop_tokens: List[str] = betterproto.string_field(7)
    max_tokens: Optional[int] = betterproto.int32_field(
        3, optional=True, group="_max_tokens"
    )
    top_p: Optional[float] = betterproto.float_field(4, optional=True, group="_top_p")
    top_k: Optional[int] = betterproto.int32_field(5, optional=True, group="_top_k")
    temperature: Optional[float] = betterproto.float_field(
        6, optional=True, group="_temperature"
    )


@dataclass(eq=False, repr=False)
class CompletionResponse(betterproto.Message):
    text: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class Model(betterproto.Message):
    architecture: str = betterproto.string_field(2)
    name: str = betterproto.string_field(1)
    tokenizer: str = betterproto.string_field(3)
    system_prompt: Optional[str] = betterproto.string_field(
        4, optional=True, group="_system_prompt"
    )
    instruction_prefix: Optional[str] = betterproto.string_field(
        5, optional=True, group="_instruction_prefix"
    )
    instruction_postfix: Optional[str] = betterproto.string_field(
        6, optional=True, group="_instruction_postfix"
    )
    output_prefix: Optional[str] = betterproto.string_field(
        7, optional=True, group="_output_prefix"
    )
    output_postfix: Optional[str] = betterproto.string_field(
        8, optional=True, group="_output_postfix"
    )


@dataclass(eq=False, repr=False)
class ModelName(betterproto.Message):
    name: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class ListModelsResponse(betterproto.Message):
    models: List["Model"] = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class Worker(betterproto.Message):
    worker_name: str = betterproto.string_field(1)
    status: "Status" = betterproto.enum_field(2)


@dataclass(eq=False, repr=False)
class ListWorkersResponse(betterproto.Message):
    workers: List["Worker"] = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class CreateInferenceWorkerRequest(betterproto.Message):
    model_name: str = betterproto.string_field(1)
    quantization: str = betterproto.string_field(2)
    worker_name: Optional[str] = betterproto.string_field(
        3, optional=True, group="_worker_name"
    )
    gpu_type: Optional["GpuType"] = betterproto.enum_field(
        4, optional=True, group="_gpu_type"
    )
    gpu_count: Optional[int] = betterproto.int32_field(
        6, optional=True, group="_gpu_count"
    )
    zone: Optional[str] = betterproto.string_field(7, optional=True, group="_zone")


@dataclass(eq=False, repr=False)
class InferenceWorker(betterproto.Message):
    worker_name: str = betterproto.string_field(1)


class HavenStub(betterproto.ServiceStub):
    async def setup(
        self,
        setup_request: "SetupRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "SetupResponse":
        return await self._unary_unary(
            "/haven.Haven/Setup",
            setup_request,
            SetupResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def chat_completion(
        self,
        chat_completion_request: "ChatCompletionRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> AsyncIterator["CompletionResponse"]:
        async for response in self._unary_stream(
            "/haven.Haven/ChatCompletion",
            chat_completion_request,
            CompletionResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        ):
            yield response

    async def completion(
        self,
        completion_request: "CompletionRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> AsyncIterator["CompletionResponse"]:
        async for response in self._unary_stream(
            "/haven.Haven/Completion",
            completion_request,
            CompletionResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        ):
            yield response

    async def list_models(
        self,
        empty: "Empty",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "ListModelsResponse":
        return await self._unary_unary(
            "/haven.Haven/ListModels",
            empty,
            ListModelsResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def add_model(
        self,
        model: "Model",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "Empty":
        return await self._unary_unary(
            "/haven.Haven/AddModel",
            model,
            Empty,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def delete_model(
        self,
        model_name: "ModelName",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "Empty":
        return await self._unary_unary(
            "/haven.Haven/DeleteModel",
            model_name,
            Empty,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def list_workers(
        self,
        empty: "Empty",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "ListWorkersResponse":
        return await self._unary_unary(
            "/haven.Haven/ListWorkers",
            empty,
            ListWorkersResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def create_inference_worker(
        self,
        create_inference_worker_request: "CreateInferenceWorkerRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "InferenceWorker":
        return await self._unary_unary(
            "/haven.Haven/CreateInferenceWorker",
            create_inference_worker_request,
            InferenceWorker,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def pause_inference_worker(
        self,
        inference_worker: "InferenceWorker",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "InferenceWorker":
        return await self._unary_unary(
            "/haven.Haven/PauseInferenceWorker",
            inference_worker,
            InferenceWorker,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def resume_inference_worker(
        self,
        inference_worker: "InferenceWorker",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "InferenceWorker":
        return await self._unary_unary(
            "/haven.Haven/ResumeInferenceWorker",
            inference_worker,
            InferenceWorker,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def delete_inference_worker(
        self,
        inference_worker: "InferenceWorker",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "InferenceWorker":
        return await self._unary_unary(
            "/haven.Haven/DeleteInferenceWorker",
            inference_worker,
            InferenceWorker,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )


class HavenBase(ServiceBase):
    async def setup(self, setup_request: "SetupRequest") -> "SetupResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def chat_completion(
        self, chat_completion_request: "ChatCompletionRequest"
    ) -> AsyncIterator["CompletionResponse"]:
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def completion(
        self, completion_request: "CompletionRequest"
    ) -> AsyncIterator["CompletionResponse"]:
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def list_models(self, empty: "Empty") -> "ListModelsResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def add_model(self, model: "Model") -> "Empty":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def delete_model(self, model_name: "ModelName") -> "Empty":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def list_workers(self, empty: "Empty") -> "ListWorkersResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def create_inference_worker(
        self, create_inference_worker_request: "CreateInferenceWorkerRequest"
    ) -> "InferenceWorker":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def pause_inference_worker(
        self, inference_worker: "InferenceWorker"
    ) -> "InferenceWorker":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def resume_inference_worker(
        self, inference_worker: "InferenceWorker"
    ) -> "InferenceWorker":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def delete_inference_worker(
        self, inference_worker: "InferenceWorker"
    ) -> "InferenceWorker":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def __rpc_setup(
        self, stream: "grpclib.server.Stream[SetupRequest, SetupResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.setup(request)
        await stream.send_message(response)

    async def __rpc_chat_completion(
        self, stream: "grpclib.server.Stream[ChatCompletionRequest, CompletionResponse]"
    ) -> None:
        request = await stream.recv_message()
        await self._call_rpc_handler_server_stream(
            self.chat_completion,
            stream,
            request,
        )

    async def __rpc_completion(
        self, stream: "grpclib.server.Stream[CompletionRequest, CompletionResponse]"
    ) -> None:
        request = await stream.recv_message()
        await self._call_rpc_handler_server_stream(
            self.completion,
            stream,
            request,
        )

    async def __rpc_list_models(
        self, stream: "grpclib.server.Stream[Empty, ListModelsResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.list_models(request)
        await stream.send_message(response)

    async def __rpc_add_model(
        self, stream: "grpclib.server.Stream[Model, Empty]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.add_model(request)
        await stream.send_message(response)

    async def __rpc_delete_model(
        self, stream: "grpclib.server.Stream[ModelName, Empty]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.delete_model(request)
        await stream.send_message(response)

    async def __rpc_list_workers(
        self, stream: "grpclib.server.Stream[Empty, ListWorkersResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.list_workers(request)
        await stream.send_message(response)

    async def __rpc_create_inference_worker(
        self,
        stream: "grpclib.server.Stream[CreateInferenceWorkerRequest, InferenceWorker]",
    ) -> None:
        request = await stream.recv_message()
        response = await self.create_inference_worker(request)
        await stream.send_message(response)

    async def __rpc_pause_inference_worker(
        self, stream: "grpclib.server.Stream[InferenceWorker, InferenceWorker]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.pause_inference_worker(request)
        await stream.send_message(response)

    async def __rpc_resume_inference_worker(
        self, stream: "grpclib.server.Stream[InferenceWorker, InferenceWorker]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.resume_inference_worker(request)
        await stream.send_message(response)

    async def __rpc_delete_inference_worker(
        self, stream: "grpclib.server.Stream[InferenceWorker, InferenceWorker]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.delete_inference_worker(request)
        await stream.send_message(response)

    def __mapping__(self) -> Dict[str, grpclib.const.Handler]:
        return {
            "/haven.Haven/Setup": grpclib.const.Handler(
                self.__rpc_setup,
                grpclib.const.Cardinality.UNARY_UNARY,
                SetupRequest,
                SetupResponse,
            ),
            "/haven.Haven/ChatCompletion": grpclib.const.Handler(
                self.__rpc_chat_completion,
                grpclib.const.Cardinality.UNARY_STREAM,
                ChatCompletionRequest,
                CompletionResponse,
            ),
            "/haven.Haven/Completion": grpclib.const.Handler(
                self.__rpc_completion,
                grpclib.const.Cardinality.UNARY_STREAM,
                CompletionRequest,
                CompletionResponse,
            ),
            "/haven.Haven/ListModels": grpclib.const.Handler(
                self.__rpc_list_models,
                grpclib.const.Cardinality.UNARY_UNARY,
                Empty,
                ListModelsResponse,
            ),
            "/haven.Haven/AddModel": grpclib.const.Handler(
                self.__rpc_add_model,
                grpclib.const.Cardinality.UNARY_UNARY,
                Model,
                Empty,
            ),
            "/haven.Haven/DeleteModel": grpclib.const.Handler(
                self.__rpc_delete_model,
                grpclib.const.Cardinality.UNARY_UNARY,
                ModelName,
                Empty,
            ),
            "/haven.Haven/ListWorkers": grpclib.const.Handler(
                self.__rpc_list_workers,
                grpclib.const.Cardinality.UNARY_UNARY,
                Empty,
                ListWorkersResponse,
            ),
            "/haven.Haven/CreateInferenceWorker": grpclib.const.Handler(
                self.__rpc_create_inference_worker,
                grpclib.const.Cardinality.UNARY_UNARY,
                CreateInferenceWorkerRequest,
                InferenceWorker,
            ),
            "/haven.Haven/PauseInferenceWorker": grpclib.const.Handler(
                self.__rpc_pause_inference_worker,
                grpclib.const.Cardinality.UNARY_UNARY,
                InferenceWorker,
                InferenceWorker,
            ),
            "/haven.Haven/ResumeInferenceWorker": grpclib.const.Handler(
                self.__rpc_resume_inference_worker,
                grpclib.const.Cardinality.UNARY_UNARY,
                InferenceWorker,
                InferenceWorker,
            ),
            "/haven.Haven/DeleteInferenceWorker": grpclib.const.Handler(
                self.__rpc_delete_inference_worker,
                grpclib.const.Cardinality.UNARY_UNARY,
                InferenceWorker,
                InferenceWorker,
            ),
        }
